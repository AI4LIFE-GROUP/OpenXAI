{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "from openxai.experiment_utils import print_summary\n",
    "\n",
    "# ML models\n",
    "from openxai.model import LoadModel\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders\n",
    "\n",
    "# Explanation models\n",
    "from openxai.explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator\n",
    "\n",
    "# Perturbation methods required for the computation of the relative stability metrics\n",
    "from openxai.explainers.catalog.perturbation_methods import NormalPerturbation\n",
    "from openxai.explainers.catalog.perturbation_methods import NewDiscrete_NormalPerturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "data_loader_batch_size = 32\n",
    "data_name = 'adult' # must be one of ['adult', 'compas', 'gaussian', 'german', 'gmsc', 'heart', 'heloc', 'pima']\n",
    "model_name = 'lr'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Explanation method hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Lime\n",
    "lime_mode = 'tabular'\n",
    "lime_sample_around_instance = True\n",
    "lime_kernel_width = 0.75\n",
    "lime_n_samples = 1000\n",
    "lime_discretize_continuous = False\n",
    "lime_standard_deviation = float(np.sqrt(0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "loader_train, loader_test = return_loaders(data_name=data_name,\n",
    "                                           download=True,\n",
    "                                           batch_size=data_loader_batch_size)\n",
    "data_iter = iter(loader_test)\n",
    "inputs, labels = next(data_iter)\n",
    "labels = labels.type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full training data set\n",
    "data_all = torch.FloatTensor(loader_train.dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load a pretrained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Class 1:\n",
      "\tTest Preds:\t0.2075\n",
      "\tTest Set:\t0.2479\n",
      "Test Accuracy: 0.8325\n",
      "Train Accuracy: 0.8349\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ml model\n",
    "model = LoadModel(data_name=data_name,\n",
    "                  ml_model=model_name,\n",
    "                  pretrained=True)\n",
    "print_summary(model, loader_train, loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Choose an explanation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I: Explanation method with particular hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can supply your own set of hyperparameters like so:\n",
    "param_dict_lime = dict()\n",
    "param_dict_lime['dataset_tensor'] = data_all\n",
    "param_dict_lime['std'] = lime_standard_deviation\n",
    "param_dict_lime['mode'] = lime_mode\n",
    "param_dict_lime['sample_around_instance'] = lime_sample_around_instance\n",
    "param_dict_lime['kernel_width'] = lime_kernel_width\n",
    "param_dict_lime['n_samples'] = lime_n_samples\n",
    "param_dict_lime['discretize_continuous'] = lime_discretize_continuous\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=param_dict_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_custom = lime.get_explanation(inputs, \n",
    "                                   label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0330, -0.0051,  0.1004,  0.2947,  0.0746,  0.0352, -0.0251, -0.0039,\n",
       "        -0.0590, -0.0255, -0.0299,  0.0066,  0.0038])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_custom[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II: Explanation method with default hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0417,  0.0058,  0.1477,  0.4515,  0.0812,  0.0559, -0.0368,  0.0054,\n",
       "        -0.0855, -0.0301, -0.0355, -0.0088, -0.0063])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use the default hyperparameters likes so:\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=None)\n",
    "lime_default_exp = lime.get_explanation(inputs.float(), \n",
    "                                        label=labels)\n",
    "lime_default_exp[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III: Explanation method with default hyperparameters (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0358, -0.0087, -0.1244, -0.3833, -0.0815, -0.0415,  0.0282,  0.0016,\n",
       "         0.0803,  0.0216,  0.0283,  0.0006, -0.0024], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 5\n",
    "# To use a different explanation method change the method name like so\n",
    "ig = Explainer(method='ig',\n",
    "               model=model,\n",
    "               dataset_tensor=data_all,\n",
    "               param_dict_lime=None)\n",
    "ig_default_exp = ig.get_explanation(inputs.float(), \n",
    "                                    label=labels)\n",
    "ig_default_exp[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Choose an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openxai.experiment_utils import generate_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation class parameters\n",
    "perturbation_mean = 0.0\n",
    "perturbation_std = 0.10\n",
    "perturbation_flip_percentage = 0.03\n",
    "if data_name == 'compas':\n",
    "    feature_types = ['c', 'd', 'c', 'c', 'd', 'd', 'd']\n",
    "# Adult feature types\n",
    "elif data_name == 'adult':\n",
    "    feature_types = ['c'] * 6 + ['d'] * 7\n",
    "\n",
    "# Gaussian feature types\n",
    "elif data_name == 'synthetic':\n",
    "    feature_types = ['c'] * 20\n",
    "# Heloc feature types\n",
    "elif data_name == 'heloc':\n",
    "    feature_types = ['c'] * 23\n",
    "elif data_name == 'german':\n",
    "    feature_types = pickle.load(open('./data/German_Credit_Data/german-feature-metadata.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation methods\n",
    "if data_name == 'german':\n",
    "    # use special perturbation class\n",
    "    perturbation = NewDiscrete_NormalPerturbation(\"tabular\",\n",
    "                                                  mean=perturbation_mean,\n",
    "                                                  std_dev=perturbation_std,\n",
    "                                                  flip_percentage=perturbation_flip_percentage)\n",
    "\n",
    "else:\n",
    "    perturbation = NormalPerturbation(\"tabular\",\n",
    "                                      mean=perturbation_mean,\n",
    "                                      std_dev=perturbation_std,\n",
    "                                      flip_percentage=perturbation_flip_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = dict()\n",
    "index = index\n",
    "\n",
    "# inputs and models\n",
    "input_dict['x'] = inputs[index].reshape(-1)\n",
    "input_dict['input_data'] = inputs\n",
    "input_dict['explainer'] = ig\n",
    "input_dict['explanation_x'] = ig_default_exp[index,:].flatten()\n",
    "input_dict['model'] = model\n",
    "\n",
    "# perturbation method used for the stability metric\n",
    "input_dict['perturbation'] = perturbation\n",
    "input_dict['perturb_method'] = perturbation\n",
    "input_dict['perturb_max_distance'] = 0.4\n",
    "input_dict['feature_metadata'] = feature_types\n",
    "input_dict['p_norm'] = 2\n",
    "input_dict['eval_metric'] = None\n",
    "\n",
    "# true label, predicted label, and masks\n",
    "input_dict['top_k'] = 3\n",
    "input_dict['y'] = labels[index].detach().item()\n",
    "input_dict['y_pred'] = torch.max(model(inputs[index].unsqueeze(0).float()), 1).indices.detach().item()\n",
    "input_dict['mask'] = generate_mask(input_dict['explanation_x'].reshape(-1), input_dict['top_k'])\n",
    "\n",
    "# required for the representation stability measure\n",
    "input_dict['L_map'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(input_dict,\n",
    "                      inputs=inputs,\n",
    "                      labels=labels, \n",
    "                      model=model, \n",
    "                      explainer=ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RC: (array([1.]), 1.0)\n",
      "FA: (array([1.]), 1.0)\n",
      "RA: (array([1.]), 1.0)\n",
      "SA: (array([0.]), 0.0)\n",
      "SRA: (array([0.]), 0.0)\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'return_ground_truth_importance'):\n",
    "    # evaluate rank correlation\n",
    "    print('RC:', evaluator.evaluate(metric='RC'))\n",
    "\n",
    "    # evaluate feature agreement\n",
    "    print('FA:', evaluator.evaluate(metric='FA'))\n",
    "\n",
    "    # evaluate rank agreement\n",
    "    print('RA:', evaluator.evaluate(metric='RA'))\n",
    "\n",
    "    # evaluate sign agreement\n",
    "    print('SA:', evaluator.evaluate(metric='SA'))\n",
    "\n",
    "    # evaluate signed rankcorrelation\n",
    "    print('SRA:', evaluator.evaluate(metric='SRA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGU: 0.011302679\n",
      "PGI: 0.035174333\n",
      "RIS: 17.546217812948438\n",
      "ROS: 298.6032831337571\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction gap on umportant features\n",
    "print('PGU:', evaluator.evaluate(metric='PGU'))\n",
    "\n",
    "# evaluate prediction gap on important features\n",
    "print('PGI:', evaluator.evaluate(metric='PGI'))\n",
    "\n",
    "# evaluate relative input stability\n",
    "print('RIS:', evaluator.evaluate(metric='RIS'))\n",
    "\n",
    "# evaluate relative output stability\n",
    "print('ROS:', evaluator.evaluate(metric='ROS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
