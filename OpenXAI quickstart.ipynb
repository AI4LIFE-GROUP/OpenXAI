{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ML models\n",
    "from openxai.LoadModel import LoadModel\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders\n",
    "\n",
    "# Explanation models\n",
    "from openxai.Explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator_final import Evaluator\n",
    "\n",
    "# Perturbation methods required for the computation of the relative stability metrics\n",
    "from openxai.explainers.catalog.perturbation_methods import NormalPerturbation\n",
    "from openxai.explainers.catalog.perturbation_methods import NewDiscrete_NormalPerturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "data_loader_batch_size = 32\n",
    "data_name = 'adult' # must be one of ['heloc', 'adult', 'german', 'compas']\n",
    "model_name = 'lr'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Explanation method hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Lime\n",
    "lime_mode = 'tabular'\n",
    "lime_sample_around_instance = True\n",
    "lime_kernel_width = 0.75\n",
    "lime_n_samples = 1000\n",
    "lime_discretize_continuous = False\n",
    "lime_standard_deviation = float(np.sqrt(0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "loader_train, loader_test = return_loaders(data_name=data_name,\n",
    "                                           download=True,\n",
    "                                           batch_size=data_loader_batch_size)\n",
    "data_iter = iter(loader_test)\n",
    "inputs, labels = data_iter.next()\n",
    "labels = labels.type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full training data set\n",
    "data_all = torch.FloatTensor(loader_train.dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load a pretrained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ml model\n",
    "model = LoadModel(data_name=data_name,\n",
    "                  ml_model=model_name,\n",
    "                  pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Choose an explanation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I: Explanation method with particular hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can supply your own set of hyperparameters like so:\n",
    "param_dict_lime = dict()\n",
    "param_dict_lime['dataset_tensor'] = data_all\n",
    "param_dict_lime['std'] = lime_standard_deviation\n",
    "param_dict_lime['mode'] = lime_mode\n",
    "param_dict_lime['sample_around_instance'] = lime_sample_around_instance\n",
    "param_dict_lime['kernel_width'] = lime_kernel_width\n",
    "param_dict_lime['n_samples'] = lime_n_samples\n",
    "param_dict_lime['discretize_continuous'] = lime_discretize_continuous\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=param_dict_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 103.17it/s]\n"
     ]
    }
   ],
   "source": [
    "lime_custom = lime.get_explanation(inputs, \n",
    "                                   label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1570,  0.0026,  0.3484,  1.9900,  0.2199,  0.1951,  0.0260,  0.0037,\n",
       "        -0.2435, -0.0200,  0.0290, -0.0160,  0.0146])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_custom[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II: Explanation method with default hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 105.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1172,  0.0181,  0.3022,  1.6208,  0.1773,  0.1373,  0.0312,  0.0148,\n",
       "        -0.1573, -0.0393,  0.0227,  0.0079,  0.0043])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use the default hyperparameters likes so:\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=None)\n",
    "lime_default_exp = lime.get_explanation(inputs.float(), \n",
    "                                        label=labels)\n",
    "lime_default_exp[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III: Explanation method with default hyperparameters (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0408e-01, -3.9771e-02, -7.7087e-01, -4.1969e+00, -4.6607e-01,\n",
       "        -3.9651e-01, -4.8253e-02,  8.1945e-04,  4.8151e-01,  1.0392e-01,\n",
       "        -4.9245e-02, -1.4125e-02, -8.8453e-03], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use a different explanation method change the method name like so\n",
    "ig = Explainer(method='ig',\n",
    "               model=model,\n",
    "               dataset_tensor=data_all,\n",
    "               param_dict_lime=None)\n",
    "ig_default_exp = ig.get_explanation(inputs.float(), \n",
    "                                    label=labels)\n",
    "ig_default_exp[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Choose an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(explanation, top_k):\n",
    "    mask_indices = torch.topk(explanation, top_k).indices\n",
    "    mask = torch.zeros(explanation.shape) > 10\n",
    "    for i in mask_indices:\n",
    "        mask[i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation class parameters\n",
    "perturbation_mean = 0.0\n",
    "perturbation_std = 0.10\n",
    "perturbation_flip_percentage = 0.03\n",
    "if data_name == 'compas':\n",
    "    feature_types = ['c', 'd', 'c', 'c', 'd', 'd', 'd']\n",
    "# Adult feature types\n",
    "elif data_name == 'adult':\n",
    "    feature_types = ['c'] * 6 + ['d'] * 7\n",
    "\n",
    "# Gaussian feature types\n",
    "elif data_name == 'synthetic':\n",
    "    feature_types = ['c'] * 20\n",
    "# Heloc feature types\n",
    "elif data_name == 'heloc':\n",
    "    feature_types = ['c'] * 23\n",
    "elif data_name == 'german':\n",
    "    feature_types = pickle.load(open('./data/German_Credit_Data/german-feature-metadata.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation methods\n",
    "if data_name == 'german':\n",
    "    # use special perturbation class\n",
    "    perturbation = NewDiscrete_NormalPerturbation(\"tabular\",\n",
    "                                                  mean=perturbation_mean,\n",
    "                                                  std_dev=perturbation_std,\n",
    "                                                  flip_percentage=perturbation_flip_percentage)\n",
    "\n",
    "else:\n",
    "    perturbation = NormalPerturbation(\"tabular\",\n",
    "                                      mean=perturbation_mean,\n",
    "                                      std_dev=perturbation_std,\n",
    "                                      flip_percentage=perturbation_flip_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = dict()\n",
    "index = 1\n",
    "\n",
    "# inputs and models\n",
    "input_dict['x'] = inputs[index].reshape(-1)\n",
    "input_dict['input_data'] = inputs\n",
    "input_dict['explainer'] = ig\n",
    "input_dict['explanation_x'] = ig_default_exp[index,:].flatten()\n",
    "input_dict['model'] = model\n",
    "\n",
    "# perturbation method used for the stability metric\n",
    "input_dict['perturbation'] = perturbation\n",
    "input_dict['perturb_method'] = perturbation\n",
    "input_dict['perturb_max_distance'] = 0.4\n",
    "input_dict['feature_metadata'] = feature_types\n",
    "input_dict['p_norm'] = 2\n",
    "input_dict['eval_metric'] = None\n",
    "\n",
    "# true label, predicted label, and masks\n",
    "input_dict['top_k'] = 6\n",
    "input_dict['y'] = labels[index].detach().item()\n",
    "input_dict['y_pred'] = torch.max(model(inputs[index].unsqueeze(0).float()), 1).indices.detach().item()\n",
    "input_dict['mask'] = generate_mask(input_dict['explanation_x'].reshape(-1), input_dict['top_k'])\n",
    "\n",
    "# required for the representation stability measure\n",
    "input_dict['L_map'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(input_dict,\n",
    "                      inputs=inputs,\n",
    "                      labels=labels, \n",
    "                      model=model, \n",
    "                      explainer=ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RC: (array([1.]), 0.9999999999999999)\n",
      "FA: (array([1.]), 1.0)\n",
      "RA: (array([1.]), 1.0)\n",
      "SA: (array([0.]), 0.0)\n",
      "SRA: (array([0.]), 0.0)\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'return_ground_truth_importance'):\n",
    "    # evaluate rank correlation\n",
    "    print('RC:', evaluator.evaluate(metric='RC'))\n",
    "\n",
    "    # evaluate feature agreement\n",
    "    print('FA:', evaluator.evaluate(metric='FA'))\n",
    "\n",
    "    # evaluate rank agreement\n",
    "    print('RA:', evaluator.evaluate(metric='RA'))\n",
    "\n",
    "    # evaluate sign agreement\n",
    "    print('SA:', evaluator.evaluate(metric='SA'))\n",
    "\n",
    "    # evaluate signed rankcorrelation\n",
    "    print('SRA:', evaluator.evaluate(metric='SRA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGU: 0.23780215\n",
      "PGI: 0.7303029\n",
      "RIS: 32.08356894944248\n",
      "ROS: 2702.5978517349345\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction gap on umportant features\n",
    "print('PGU:', evaluator.evaluate(metric='PGU'))\n",
    "\n",
    "# evaluate prediction gap on important features\n",
    "print('PGI:', evaluator.evaluate(metric='PGI'))\n",
    "\n",
    "# evaluate relative input stability\n",
    "print('RIS:', evaluator.evaluate(metric='RIS'))\n",
    "\n",
    "# evaluate relative output stability\n",
    "print('ROS:', evaluator.evaluate(metric='ROS'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
