{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# ML models\n",
    "from openxai.LoadModel import LoadModel\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders\n",
    "\n",
    "# Explanation models\n",
    "from openxai.Explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator\n",
    "\n",
    "# Perturbation methods required for the computation of the relative stability metrics\n",
    "from openxai.explainers.catalog.perturbation_methods import NormalPerturbation\n",
    "from openxai.explainers.catalog.perturbation_methods import NewDiscrete_NormalPerturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "data_loader_batch_size = 32\n",
    "data_name = 'adult' # must be one of ['heloc', 'adult', 'german', 'compas']\n",
    "model_name = 'lr'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Explanation method hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Lime\n",
    "lime_mode = 'tabular'\n",
    "lime_sample_around_instance = True\n",
    "lime_kernel_width = 0.75\n",
    "lime_n_samples = 1000\n",
    "lime_discretize_continuous = False\n",
    "lime_standard_deviation = float(np.sqrt(0.03))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "loader_train, loader_test = return_loaders(data_name=data_name,\n",
    "                                           download=True,\n",
    "                                           batch_size=data_loader_batch_size)\n",
    "data_iter = iter(loader_test)\n",
    "inputs, labels = data_iter.next()\n",
    "labels = labels.type(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full training data set\n",
    "data_all = torch.FloatTensor(loader_train.dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load a pretrained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ml model\n",
    "model = LoadModel(data_name=data_name,\n",
    "                  ml_model=model_name,\n",
    "                  pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Choose an explanation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I: Explanation method with particular hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can supply your own set of hyperparameters like so:\n",
    "param_dict_lime = dict()\n",
    "param_dict_lime['dataset_tensor'] = data_all\n",
    "param_dict_lime['std'] = lime_standard_deviation\n",
    "param_dict_lime['mode'] = lime_mode\n",
    "param_dict_lime['sample_around_instance'] = lime_sample_around_instance\n",
    "param_dict_lime['kernel_width'] = lime_kernel_width\n",
    "param_dict_lime['n_samples'] = lime_n_samples\n",
    "param_dict_lime['discretize_continuous'] = lime_discretize_continuous\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=param_dict_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 109.01it/s]\n"
     ]
    }
   ],
   "source": [
    "lime_custom = lime.get_explanation(inputs, \n",
    "                                   label=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1347,  0.0174,  0.3678,  1.8841,  0.2197,  0.1585,  0.0202, -0.0109,\n",
       "        -0.2072, -0.0564,  0.0281,  0.0156,  0.0224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_custom[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II: Explanation method with default hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 97.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1265,  0.0020,  0.2932,  1.5496,  0.1593,  0.1359,  0.0123, -0.0041,\n",
       "        -0.2057, -0.0356,  0.0139, -0.0087,  0.0050])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use the default hyperparameters likes so:\n",
    "lime = Explainer(method='lime',\n",
    "                 model=model,\n",
    "                 dataset_tensor=data_all,\n",
    "                 param_dict_lime=None)\n",
    "lime_default_exp = lime.get_explanation(inputs.float(), \n",
    "                                        label=labels)\n",
    "lime_default_exp[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III: Explanation method with default hyperparameters (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0326e-01, -1.3506e-02, -2.6178e-01, -1.4252e+00, -1.5827e-01,\n",
       "        -1.3465e-01, -1.6386e-02,  2.7828e-04,  1.6352e-01,  3.5290e-02,\n",
       "        -1.6723e-02, -4.7965e-03, -3.0037e-03], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 5\n",
    "# To use a different explanation method change the method name like so\n",
    "ig = Explainer(method='ig',\n",
    "               model=model,\n",
    "               dataset_tensor=data_all,\n",
    "               param_dict_lime=None)\n",
    "ig_default_exp = ig.get_explanation(inputs.float(), \n",
    "                                    label=labels)\n",
    "ig_default_exp[index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Choose an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(explanation, top_k):\n",
    "    mask_indices = torch.topk(explanation, top_k).indices\n",
    "    mask = torch.zeros(explanation.shape) > 10\n",
    "    for i in mask_indices:\n",
    "        mask[i] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation class parameters\n",
    "perturbation_mean = 0.0\n",
    "perturbation_std = 0.10\n",
    "perturbation_flip_percentage = 0.03\n",
    "if data_name == 'compas':\n",
    "    feature_types = ['c', 'd', 'c', 'c', 'd', 'd', 'd']\n",
    "# Adult feature types\n",
    "elif data_name == 'adult':\n",
    "    feature_types = ['c'] * 6 + ['d'] * 7\n",
    "\n",
    "# Gaussian feature types\n",
    "elif data_name == 'synthetic':\n",
    "    feature_types = ['c'] * 20\n",
    "# Heloc feature types\n",
    "elif data_name == 'heloc':\n",
    "    feature_types = ['c'] * 23\n",
    "elif data_name == 'german':\n",
    "    feature_types = pickle.load(open('./data/German_Credit_Data/german-feature-metadata.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturbation methods\n",
    "if data_name == 'german':\n",
    "    # use special perturbation class\n",
    "    perturbation = NewDiscrete_NormalPerturbation(\"tabular\",\n",
    "                                                  mean=perturbation_mean,\n",
    "                                                  std_dev=perturbation_std,\n",
    "                                                  flip_percentage=perturbation_flip_percentage)\n",
    "\n",
    "else:\n",
    "    perturbation = NormalPerturbation(\"tabular\",\n",
    "                                      mean=perturbation_mean,\n",
    "                                      std_dev=perturbation_std,\n",
    "                                      flip_percentage=perturbation_flip_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = dict()\n",
    "index = index\n",
    "\n",
    "# inputs and models\n",
    "input_dict['x'] = inputs[index].reshape(-1)\n",
    "input_dict['input_data'] = inputs\n",
    "input_dict['explainer'] = ig\n",
    "input_dict['explanation_x'] = ig_default_exp[index,:].flatten()\n",
    "input_dict['model'] = model\n",
    "\n",
    "# perturbation method used for the stability metric\n",
    "input_dict['perturbation'] = perturbation\n",
    "input_dict['perturb_method'] = perturbation\n",
    "input_dict['perturb_max_distance'] = 0.4\n",
    "input_dict['feature_metadata'] = feature_types\n",
    "input_dict['p_norm'] = 2\n",
    "input_dict['eval_metric'] = None\n",
    "\n",
    "# true label, predicted label, and masks\n",
    "input_dict['top_k'] = 3\n",
    "input_dict['y'] = labels[index].detach().item()\n",
    "input_dict['y_pred'] = torch.max(model(inputs[index].unsqueeze(0).float()), 1).indices.detach().item()\n",
    "input_dict['mask'] = generate_mask(input_dict['explanation_x'].reshape(-1), input_dict['top_k'])\n",
    "\n",
    "# required for the representation stability measure\n",
    "input_dict['L_map'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(input_dict,\n",
    "                      inputs=inputs,\n",
    "                      labels=labels, \n",
    "                      model=model, \n",
    "                      explainer=ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RC: (array([1.]), 0.9999999999999999)\n",
      "FA: (array([1.]), 1.0)\n",
      "RA: (array([1.]), 1.0)\n",
      "SA: (array([0.]), 0.0)\n",
      "SRA: (array([0.]), 0.0)\n"
     ]
    }
   ],
   "source": [
    "if hasattr(model, 'return_ground_truth_importance'):\n",
    "    # evaluate rank correlation\n",
    "    print('RC:', evaluator.evaluate(metric='RC'))\n",
    "\n",
    "    # evaluate feature agreement\n",
    "    print('FA:', evaluator.evaluate(metric='FA'))\n",
    "\n",
    "    # evaluate rank agreement\n",
    "    print('RA:', evaluator.evaluate(metric='RA'))\n",
    "\n",
    "    # evaluate sign agreement\n",
    "    print('SA:', evaluator.evaluate(metric='SA'))\n",
    "\n",
    "    # evaluate signed rankcorrelation\n",
    "    print('SRA:', evaluator.evaluate(metric='SRA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGU: 0.017170012\n",
      "PGI: 0.6956483\n",
      "RIS: 20.658200855615895\n",
      "ROS: 929.0251167251357\n"
     ]
    }
   ],
   "source": [
    "# evaluate prediction gap on umportant features\n",
    "print('PGU:', evaluator.evaluate(metric='PGU'))\n",
    "\n",
    "# evaluate prediction gap on important features\n",
    "print('PGI:', evaluator.evaluate(metric='PGI'))\n",
    "\n",
    "# evaluate relative input stability\n",
    "print('RIS:', evaluator.evaluate(metric='RIS'))\n",
    "\n",
    "# evaluate relative output stability\n",
    "print('ROS:', evaluator.evaluate(metric='ROS'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
