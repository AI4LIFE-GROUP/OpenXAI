{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This notebook contains a quickstart guide to using OpenXAI, covering:\n",
    "- `(0) Preamble and data/model selection`\n",
    "- `(1) Loading preprocessed datasets`\n",
    "- `(2) Loading pretrained models`\n",
    "- `(3) Configuring and using explanation methods`\n",
    "- `(4) Configuring and using evaluation metrics on the explanations generated`\n",
    "\n",
    "##### Full model training, explanation, and evaluation pipelines can be found at:\n",
    "- `train_models.py`\n",
    "- `generate_explanations.py`\n",
    "- `evaluate_metrics.py`\n",
    "\n",
    "##### Each of which parses parameters from:\n",
    "- `experiment_config.json`\n",
    "\n",
    "*This notebook is a lightweight alternative to the full pipelines, intended for users who want to get started quickly or customize their own pipeline.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0) Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import torch\n",
    "import numpy as np\n",
    "from openxai.experiment_utils import print_summary, load_config, fill_param_dict\n",
    "from openxai.explainers.perturbation_methods import get_perturb_method\n",
    "\n",
    "# ML models\n",
    "from openxai.model import LoadModel\n",
    "\n",
    "# Data loaders\n",
    "from openxai.dataloader import return_loaders, return_train_test_inputs\n",
    "\n",
    "# Explanation models\n",
    "from openxai.explainer import Explainer\n",
    "\n",
    "# Evaluation methods\n",
    "from openxai.evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model and the data set you wish to generate explanations for\n",
    "n_test_samples = 10\n",
    "data_name = 'adult' # must be one of ['adult', 'compas', 'gaussian', 'german', 'gmsc', 'heart', 'heloc', 'pima']\n",
    "model_name = 'lr'    # must be one of ['lr', 'ann']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loaders\n",
    "trainloader, testloader = return_loaders(data_name=data_name,\n",
    "                                           download=True,\n",
    "                                           batch_size=n_test_samples)\n",
    "inputs, labels = next(iter(testloader))\n",
    "labels = labels.type(torch.int64)\n",
    "\n",
    "# Get full train/test FloatTensors and feature metadata\n",
    "X_train, X_test, feature_metadata = return_train_test_inputs(data_name, float_tensor=True, return_feature_metadata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Load a pretrained ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Class 1:\n",
      "\tTest Preds:\t0.2075\n",
      "\tTest Set:\t0.2479\n",
      "Test Accuracy: 0.8325\n",
      "Train Accuracy: 0.8349\n",
      "First 10 predictions: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained ml model\n",
    "model = LoadModel(data_name=data_name,\n",
    "                  ml_model=model_name,\n",
    "                  pretrained=True)\n",
    "print_summary(model, trainloader, testloader)\n",
    "preds = model(inputs.float()).argmax(1)\n",
    "print(f'First 10 predictions: {preds[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Choose an explanation method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Explanation method with config hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME Parameters\n",
      "\n",
      "n_samples: 1000\n",
      "kernel_width: 0.75\n",
      "std: 0.1\n",
      "mode: tabular\n",
      "sample_around_instance: True\n",
      "discretize_continuous: False\n",
      "seed: 0\n",
      "data: tensor([[0.4384, 0.1733, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.1233, 0.1169, 0.5333,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.2877, 0.0135, 0.5333,  ..., 0.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [0.1096, 0.2245, 0.3333,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.4658, 0.0893, 0.8000,  ..., 0.0000, 1.0000, 1.0000],\n",
      "        [0.2877, 0.0263, 0.8000,  ..., 1.0000, 1.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'lime'\n",
    "\n",
    "# Load config parameters for the explainer\n",
    "param_dict = load_config('experiment_config.json')['explainers'][method]\n",
    "\n",
    "# # If LIME/IG, then provide X_train\n",
    "param_dict = fill_param_dict(method, param_dict, X_train)\n",
    "print(f'{method.upper()} Parameters\\n\\n' +'\\n'.join([f'{k}: {v}' for k, v in param_dict.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0011376  -0.00234992 -0.01240377 -0.03825278 -0.01005829  0.00205742\n",
      "  0.00217722  0.00086493  0.00163225 -0.00117567  0.00221764  0.00232039\n",
      " -0.00024308]\n"
     ]
    }
   ],
   "source": [
    "# Compute explanations\n",
    "lime = Explainer(method, model, param_dict)\n",
    "lime_exps = lime.get_explanation(inputs, preds).detach().numpy()\n",
    "print(lime_exps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II: Explanation method with default hyperparameters (LIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIME Parameters\n",
      "\n",
      "data: tensor([[0.4384, 0.1733, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.1233, 0.1169, 0.5333,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.2877, 0.0135, 0.5333,  ..., 0.0000, 1.0000, 1.0000],\n",
      "        ...,\n",
      "        [0.1096, 0.2245, 0.3333,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.4658, 0.0893, 0.8000,  ..., 0.0000, 1.0000, 1.0000],\n",
      "        [0.2877, 0.0263, 0.8000,  ..., 1.0000, 1.0000, 1.0000]])\n",
      "Remaining parameters are set to their default values\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'lime'\n",
    "\n",
    "# Pass empty dict to use default parameters\n",
    "param_dict = {}\n",
    "\n",
    "# # If LIME/IG, then provide X_train\n",
    "param_dict = fill_param_dict(method, {}, X_train)\n",
    "print(f'{method.upper()} Parameters\\n\\n' +'\\n'.join([f'{k}: {v}' for k, v in param_dict.items()]))\n",
    "print('Remaining parameters are set to their default values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01484873 -0.00360259 -0.05157194 -0.15891102 -0.03377749 -0.01719308\n",
      "  0.01170494  0.00064746  0.03330764  0.00893782  0.01174643  0.00024131\n",
      " -0.00099279]\n"
     ]
    }
   ],
   "source": [
    "# Compute explanations\n",
    "lime = Explainer(method, model, param_dict)\n",
    "lime_exps = lime.get_explanation(inputs.float(), preds).detach().numpy()\n",
    "print(lime_exps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III: Explanation method with default hyperparameters (IG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01484873 -0.00360259 -0.05157194 -0.15891102 -0.03377749 -0.01719308\n",
      "  0.01170494  0.00064746  0.03330764  0.00893782  0.01174643  0.00024131\n",
      " -0.00099279]\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'ig'\n",
    "\n",
    "# If LIME/IG, then provide X_train\n",
    "param_dict = fill_param_dict('ig', {}, X_train)\n",
    "\n",
    "# Compute explanations\n",
    "ig = Explainer('ig', model, param_dict)\n",
    "ig_exps = ig.get_explanation(inputs.float(), preds).detach().numpy()\n",
    "print(ig_exps[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV: Explanation method with additional hyperparameters (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00378722 -0.01268718 -0.12357364  0.00807462  0.00349009 -0.01655754\n",
      "  0.03306515 -0.0088843   0.05668441  0.02878716  0.04120433  0.0033168\n",
      "  0.00390152]\n"
     ]
    }
   ],
   "source": [
    "# Choose explainer\n",
    "method = 'shap'\n",
    "\n",
    "# Override default parameters for certain hyperparameters\n",
    "param_dict = {'n_samples': 1000, 'seed': 0}\n",
    "\n",
    "# Compute explanations\n",
    "shap = Explainer(method, model, param_dict)\n",
    "shap_exps = shap.get_explanation(inputs.float(), preds).detach().numpy()\n",
    "print(shap_exps[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Choose an evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth metrics:  ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
      "Prediction metrics:  ['PGU', 'PGI']\n",
      "Stability metrics:  ['RIS', 'RRS', 'ROS']\n"
     ]
    }
   ],
   "source": [
    "from openxai.evaluator import ground_truth_metrics, prediction_metrics, stability_metrics\n",
    "print('Ground truth metrics: ', ground_truth_metrics)\n",
    "print('Prediction metrics: ', prediction_metrics)\n",
    "print('Stability metrics: ', stability_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Ground Truth Faithfulness Metrics (PRA, RC, FA, RA, SA, SRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FA Parameters\n",
      "\n",
      "k: 0.25\n",
      "AUC: True\n",
      "explanations: array of size (10, 13)\n",
      "predictions: array of size torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Choose one of ['PRA', 'RC', 'FA', 'RA', 'SA', 'SRA']\n",
    "metric = 'FA'  \n",
    "\n",
    "# Load config\n",
    "param_dict = load_config('experiment_config.json')['evaluators']['ground_truth_metrics']\n",
    "param_dict['explanations'] = lime_exps\n",
    "if metric in ['FA', 'RA', 'SA', 'SRA']:\n",
    "    param_dict['predictions'] = preds  # flips ground truth according to prediction\n",
    "elif metric in ['PRA', 'RC']:\n",
    "    del param_dict['k'], param_dict['AUC']  # not needed for PRA/RC\n",
    "\n",
    "# Print final parameters\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{metric.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Predictive Faithfulness Metrics (PGU, PGI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGU Parameters\n",
      "\n",
      "k: 0.25\n",
      "AUC: True\n",
      "n_samples: 100\n",
      "seed: -1\n",
      "n_jobs: -1\n",
      "inputs: array of size torch.Size([9045, 13])\n",
      "explanations: array of size (10, 13)\n",
      "feature_metadata: ['c', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd']\n",
      "perturb_method: <openxai.explainers.perturbation_methods.NormalPerturbation object at 0x291491d90>\n"
     ]
    }
   ],
   "source": [
    "# Choose one of ['PGU', 'PGI']\n",
    "metric = 'PGU'\n",
    "\n",
    "# Load config\n",
    "param_dict = load_config('experiment_config.json')['evaluators']['prediction_metrics']\n",
    "param_dict['inputs'] = X_test\n",
    "param_dict['explanations'] = lime_exps\n",
    "param_dict['feature_metadata'] = feature_metadata\n",
    "param_dict['perturb_method'] = get_perturb_method(param_dict['std'], data_name)\n",
    "del param_dict['std']\n",
    "\n",
    "# Print final parameters\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{metric.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Stability Metrics (RIS, RRS, ROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS Parameters\n",
      "\n",
      "n_samples: 1000\n",
      "n_perturbations: 100\n",
      "p_norm: 2\n",
      "seed: -1\n",
      "n_jobs: -1\n",
      "inputs: array of size torch.Size([9045, 13])\n",
      "explainer: <openxai.explainers.catalog.grad.grad.Gradient object at 0x17391ed30>\n",
      "perturb_method: <openxai.explainers.perturbation_methods.NormalPerturbation object at 0x292923280>\n",
      "feature_metadata: ['c', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'd', 'd', 'd', 'd']\n"
     ]
    }
   ],
   "source": [
    "# Choose one of ['RIS', 'RRS', 'ROS']\n",
    "metric = 'RIS'\n",
    "\n",
    "# Initialize explainer for stability metrics\n",
    "method = 'grad'\n",
    "exp_param_dict = load_config('experiment_config.json')['explainers'][method]\n",
    "exp_param_dict = fill_param_dict(method, exp_param_dict, X_train)  # if LIME/IG\n",
    "explainer = Explainer(method, model, exp_param_dict)\n",
    "\n",
    "\n",
    "# Load config\n",
    "param_dict = load_config('experiment_config.json')['evaluators']['stability_metrics']\n",
    "param_dict['inputs'] = X_test\n",
    "param_dict['explainer'] = explainer\n",
    "param_dict['perturb_method'] = get_perturb_method(param_dict['std'], data_name)\n",
    "param_dict['feature_metadata'] = feature_metadata\n",
    "del param_dict['std']\n",
    "\n",
    "# Print final parameters\n",
    "params_preview = [f'{k}: array of size {v.shape}' if hasattr(v, 'shape') else f'{k}: {v}' for k, v in param_dict.items()]\n",
    "print(f'{metric.upper()} Parameters\\n\\n' +'\\n'.join(params_preview))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Evaluate the explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the metric across the test inputs/explanations\n",
    "evaluator = Evaluator(model, metric)\n",
    "score, mean_score = evaluator.evaluate(**param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS: 42.01±0.32\n",
      "log(RIS): 3.74±-1.13\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "std_err = np.std(score) / np.sqrt(len(score))\n",
    "print(f\"{metric}: {mean_score:.2f}\\u00B1{std_err:.2f}\")\n",
    "if metric in stability_metrics:\n",
    "    log_mu, log_std = np.log(mean_score), np.log(std_err)\n",
    "    print(f\"log({metric}): {log_mu:.2f}\\u00B1{log_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
