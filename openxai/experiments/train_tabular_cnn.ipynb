{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "\n",
    "import ML_Models.ANN.model as model_ann\n",
    "import ML_Models.data_loader as loader\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from Data_Sets.Gaussian import dgp_gaussian\n",
    "\n",
    "from explainers.grad import Gradient\n",
    "from explainers.ebp import EBP\n",
    "from explainers.integrated_gradients import IntegratedGradients\n",
    "from explainers.guided_backprop import GuidedBackprop\n",
    "from explainers.smoothgrad import SmoothGrad\n",
    "from explainers.gradcam import GradCAM\n",
    "from explainers.guided_gradcam import GuidedGradCAM\n",
    "from explainers.lrp import LRP\n",
    "from explainers.input_x_gradient import InputTimesGradient\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader_Tabular(data.Dataset):\n",
    "    def __init__(self, path, filename, label, scale='minmax', gauss_params=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Load training dataset\n",
    "        :param path: string with path to training set\n",
    "        :param label: string, column name for label\n",
    "        :param scale: string; either 'minmax' or 'standard'\n",
    "        :param dict: standard params of gaussian dgp\n",
    "        :return: tensor with training data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.path = path\n",
    "        # Load Gaussian data\n",
    "        if self.path == 'gaussian':\n",
    "            if gauss_params is None:\n",
    "                gauss_params = {\n",
    "                    'n_samples': 2500,\n",
    "                    'dim': 25,\n",
    "                    'n_clusters': 10,\n",
    "                    'distance_to_center': 5,\n",
    "                    'test_size': 0.25,\n",
    "                    'upper_weight': 1,\n",
    "                    'lower_weight': -1,\n",
    "                    'seed': 564,\n",
    "                    'sigma': None,\n",
    "                    'sparsity': 0.25\n",
    "                }\n",
    "            \n",
    "            data_dict, data_dict_train, data_dict_test = dgp_gaussian.generate_gaussians(gauss_params['n_samples'],\n",
    "                                                        gauss_params['dim'],\n",
    "                                                        gauss_params['n_clusters'],\n",
    "                                                        gauss_params['distance_to_center'],\n",
    "                                                        gauss_params['test_size'],\n",
    "                                                        gauss_params['upper_weight'],\n",
    "                                                        gauss_params['lower_weight'],\n",
    "                                                        gauss_params['seed'],\n",
    "                                                        gauss_params['sigma'],\n",
    "                                                        gauss_params['sparsity']).dgp_vars()\n",
    "            \n",
    "            self.ground_truth_dict = data_dict\n",
    "            self.target = label\n",
    "            \n",
    "            if filename == 'train':\n",
    "                data_dict = data_dict_train\n",
    "            else:\n",
    "                data_dict = data_dict_test\n",
    "                \n",
    "            self.dataset = pd.DataFrame(data_dict['data'])\n",
    "            data_y = pd.DataFrame(data_dict['target'])\n",
    "            \n",
    "            names = []\n",
    "            for i in range(gauss_params['dim']):\n",
    "                name = 'x' + str(i)\n",
    "                names.append(name)\n",
    "                \n",
    "            self.dataset.columns = names\n",
    "            self.dataset['y'] = data_y\n",
    "            \n",
    "            # add additional Gaussian related aspects\n",
    "            self.probs = data_dict['probs']\n",
    "            self.masks = data_dict['masks']\n",
    "            self.weights = data_dict['weights']\n",
    "            self.masked_weights = data_dict['masked_weights']\n",
    "            self.cluster_idx = data_dict['cluster_idx']\n",
    "            \n",
    "        else:\n",
    "            self.dataset = pd.read_csv(path + filename)\n",
    "            self.target = label\n",
    "        \n",
    "        # Cleaning Routine\n",
    "\n",
    "        # Save target and predictors\n",
    "        self.X = self.dataset.drop(self.target, axis=1)\n",
    "        \n",
    "        # Save feature names\n",
    "        self.feature_names = self.X.columns.to_list()\n",
    "        self.target_name = label\n",
    "\n",
    "        # Transform data\n",
    "        if scale == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        elif scale == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "            \n",
    "        self.scaler.fit_transform(self.X)\n",
    "        \n",
    "        self.data = self.scaler.transform(self.X)\n",
    "        self.targets = self.dataset[self.target]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # select correct row with idx\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        if self.path == 'gaussian':\n",
    "            return (self.data[idx], self.targets.values[idx], self.weights[idx], self.masks[idx],\n",
    "                    self.masked_weights[idx], self.probs[idx], self.cluster_idx[idx])\n",
    "        else:\n",
    "            return (self.data[idx], self.targets.values[idx])\n",
    "\n",
    "    def get_number_of_features(self):\n",
    "        return self.data.shape[1]\n",
    "    \n",
    "    def get_number_of_instances(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "def return_loaders(data_name, is_tabular, batch_size=32, transform=None, scaler='minmax', gauss_params=None):\n",
    "    \n",
    "    if is_tabular:\n",
    "        transform = None\n",
    "    else:\n",
    "        if transform is not None:\n",
    "            transform = transform\n",
    "        else:\n",
    "            # Standard Transforms\n",
    "            if data_name == 'mnist':\n",
    "                transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "            elif data_name == 'cifar10':\n",
    "                transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "            # Not supported data sets\n",
    "            else:\n",
    "                raise ValueError\n",
    "            \n",
    "    # Dictionary\n",
    "    dict = {'mnist': ('MNIST', transform, is_tabular, None),\n",
    "            'cifar10': ('CIFAR10', transform, is_tabular, None),\n",
    "            'adult': ('Adult', transform, is_tabular, 'income'),\n",
    "            'compas': ('COMPAS', transform, is_tabular, 'risk'),\n",
    "            'german': ('German_Credit_Data', transform, is_tabular, 'credit-risk'),\n",
    "            'gaussian': ('Gaussian', transform, is_tabular, 'y')\n",
    "            }\n",
    "    \n",
    "    if dict[data_name][2]:\n",
    "        \n",
    "        if dict[data_name][0] == 'Gaussian':\n",
    "            prefix = 'gaussian'\n",
    "            file_train = 'train'\n",
    "            file_test = 'test'\n",
    "        else:\n",
    "            prefix = './Data_Sets/' + dict[data_name][0] + '/'\n",
    "            file_train = data_name + '-train.csv'\n",
    "            file_test = data_name + '-test.csv'\n",
    "\n",
    "        dataset_train = DataLoader_Tabular(path=prefix, filename=file_train, label=dict[data_name][3],\n",
    "                                           scale=scaler, gauss_params=gauss_params)\n",
    "    \n",
    "        dataset_test = DataLoader_Tabular(path=prefix, filename=file_test, label=dict[data_name][3],\n",
    "                                          scale=scaler, gauss_params=gauss_params)\n",
    "    else:\n",
    "        if data_name == 'mnist':\n",
    "            dataset_train = MNIST(root='./Data_Sets/' + dict[data_name][0] + '/', train=True, download=True,\n",
    "                                  transform=dict[data_name][1])\n",
    "            dataset_test = MNIST(root='./Data_Sets/' + dict[data_name][0] + '/', train=False, download=True,\n",
    "                                 transform=dict[data_name][1])\n",
    "    \n",
    "        elif data_name == 'cifar10':\n",
    "            dataset_train = CIFAR10(root='./Data_Sets/' + dict[data_name][0] + '/', train=True, download=True,\n",
    "                                    transform=dict[data_name][1])\n",
    "            dataset_test = CIFAR10(root='./Data_Sets/' + dict[data_name][0] + '/', train=False, download=True,\n",
    "                                   transform=dict[data_name][1])\n",
    "\n",
    "    trainloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    testloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.GAP = nn.AvgPool2d(kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32, 84)\n",
    "        self.fc2 = nn.Linear(84, 2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        #self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.GAP(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        #x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = x.view(-1, 32)\n",
    "        print(x.shape)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        #x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    def L_relu3(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = x.view(-1, 32)\n",
    "        #x = self.relu3(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader, test_loader, learning_rate, epochs, dataset,\n",
    "             adv_train_params=None):\n",
    "    \n",
    "    loaders = {'train': train_loader,\n",
    "               'test': test_loader}\n",
    "    \n",
    "    # model collector\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # declaring optimizer and loss\n",
    "    if dataset == ('mnist' or 'cifar10' or 'gaussian'):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "    # training\n",
    "    for e in range(epochs):\n",
    "        print('Epoch {}/{}'.format(e, epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "    \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            target_true = 0.0\n",
    "            predicted_true = 0.0\n",
    "            correct_true = 0.0\n",
    "\n",
    "            if dataset == 'gaussian':\n",
    "                for i, (inputs, labels, weights, masks, masked_weights, probs, cluster_idx) in enumerate(loaders[phase]):\n",
    "    \n",
    "                    inputs = inputs.to(device).view(-1, 1, 5, 5)\n",
    "                    labels = labels.to(device).type(torch.long)\n",
    "                    #labels_one_hot = torch.nn.functional.one_hot(labels)\n",
    "                    labels_one_hot = torch.stack([1-labels, labels], dim=1)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs.float()).float()\n",
    "            \n",
    "                        loss = criterion(outputs, labels_one_hot.float())\n",
    "            \n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(torch.argmax(outputs, dim=1) == labels.float())\n",
    "\n",
    "                    # compute f1 score\n",
    "                    predicted_classes = torch.argmax(outputs, dim=1) == 0\n",
    "                    target_classes = labels.float()\n",
    "                    target_true += torch.sum(target_classes == 0).float()\n",
    "                    predicted_true += torch.sum(predicted_classes).float()\n",
    "                    correct_true += torch.sum((predicted_classes*1 == target_classes) == (predicted_classes*1 == 0)).float()\n",
    "                    \n",
    "            else:\n",
    "                \n",
    "                for i, (inputs, labels) in enumerate(loaders[phase]):\n",
    "        \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).type(torch.long)\n",
    "    \n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs.float())\n",
    "                        \n",
    "                        loss = criterion(outputs, labels)\n",
    "            \n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "            \n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(torch.round(outputs) == labels.float())\n",
    "\n",
    "            recall = correct_true / target_true\n",
    "            precision = correct_true / predicted_true\n",
    "            f1_score = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "            epoch_loss = running_loss / len(loaders[phase].dataset)\n",
    "            epoch_acc = running_corrects / len(loaders[phase].dataset)\n",
    "    \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('{} F1: {:.4f} Prec: {:.4f} Recal: {:.4f}'.format(phase, f1_score, precision, recall))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print()\n",
    "\n",
    "    if adv_train_params['type'] == 'none':\n",
    "        # save vanilla model\n",
    "        torch.save(model.state_dict(best_model_wts),\n",
    "                   'ML_Models/Saved_Models/CNN/{}_lr_{}_acc_{:.2f}.pt'.format(dataset, learning_rate, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_training_params = {'type': 'none',      \n",
    "                       'parameter': None}  \n",
    "\n",
    "dataset_train = DataLoader_Tabular(path='gaussian',\n",
    "                                          filename='train', label='y')\n",
    "\n",
    "dataset_test = DataLoader_Tabular(path='gaussian',\n",
    "                                         filename='test', label='y')\n",
    "input_size = dataset_train.get_number_of_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6250, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_test.data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6250, 5, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_test.data).view(-1, 5, 5).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 1, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset_test.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = CNN()\n",
    "\n",
    "trainloader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(dataset_test, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "#training(model, trainloader, testloader, 0.002, epochs=85, dataset='gaussian', adv_train_params=adv_training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Load CNN Model\n",
    "'''\n",
    "model_path = 'ML_Models/Saved_Models/CNN/gaussian_lr_0.002_acc_0.89.pt'\n",
    "ann = CNN()\n",
    "ann.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (GAP): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  (fc1): Linear(in_features=32, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=2, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       "  (relu3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gaussian Data DGP\n",
    "'''\n",
    "gauss_train_input = testloader.dataset.ground_truth_dict\n",
    "data_iter = iter(testloader)\n",
    "inputs, labels, weights, masks, masked_weights, probs, cluster_idx = data_iter.next()\n",
    "gaussian_all = torch.FloatTensor(testloader.dataset.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.type(torch.int64)\n",
    "inputs = inputs.view(-1,1,5,5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8965,  6.8511],\n",
       "        [ 2.4575, -2.5295],\n",
       "        [-6.3613,  6.3105],\n",
       "        [ 4.6080, -4.6685],\n",
       "        [-4.9750,  4.9100],\n",
       "        [ 4.7030, -4.7091],\n",
       "        [-6.6664,  6.6219],\n",
       "        [-0.9446,  0.8390],\n",
       "        [-4.8809,  4.8108],\n",
       "        [-2.3244,  2.2378],\n",
       "        [ 2.9469, -3.0168],\n",
       "        [-5.0472,  4.9859],\n",
       "        [ 3.4925, -3.5435],\n",
       "        [ 2.5230, -2.6092],\n",
       "        [ 3.2277, -3.2917],\n",
       "        [ 6.5061, -6.4718],\n",
       "        [ 1.4567, -1.5317],\n",
       "        [ 0.5141, -0.6176],\n",
       "        [ 0.6486, -0.7642],\n",
       "        [-0.2123,  0.1325],\n",
       "        [-0.1646,  0.0587],\n",
       "        [-3.7220,  3.6326],\n",
       "        [-4.5136,  4.4452],\n",
       "        [-3.4121,  3.3181],\n",
       "        [ 3.2772, -3.3788],\n",
       "        [ 0.4101, -0.4867],\n",
       "        [ 2.2603, -2.3605],\n",
       "        [-0.7450,  0.6561],\n",
       "        [-3.1577,  3.0524],\n",
       "        [ 1.2560, -1.3403],\n",
       "        [-4.4683,  4.3957],\n",
       "        [-4.4753,  4.4075],\n",
       "        [-4.7074,  4.6326],\n",
       "        [-0.0290, -0.0594],\n",
       "        [ 5.3316, -5.4115],\n",
       "        [ 5.7982, -5.7612],\n",
       "        [ 0.0446, -0.1321],\n",
       "        [ 4.2234, -4.2514],\n",
       "        [-1.4047,  1.3187],\n",
       "        [-2.7340,  2.6390],\n",
       "        [-5.1973,  5.1186],\n",
       "        [-0.4276,  0.3482],\n",
       "        [-2.5333,  2.4485],\n",
       "        [ 4.7497, -4.8364],\n",
       "        [ 4.3285, -4.3805],\n",
       "        [ 5.1008, -5.2055],\n",
       "        [ 0.4175, -0.5014],\n",
       "        [ 3.6814, -3.7752],\n",
       "        [ 1.8228, -1.8912],\n",
       "        [-2.0387,  1.9559],\n",
       "        [ 1.5227, -1.6244],\n",
       "        [-6.6883,  6.6486],\n",
       "        [ 3.8423, -3.8719],\n",
       "        [ 6.9473, -6.9306],\n",
       "        [ 6.1056, -6.1890],\n",
       "        [-0.9858,  0.8912],\n",
       "        [-0.2797,  0.1938],\n",
       "        [-3.3890,  3.2934],\n",
       "        [-5.4052,  5.3454],\n",
       "        [ 2.4713, -2.5485],\n",
       "        [-0.2088,  0.1045],\n",
       "        [ 4.8662, -4.9682],\n",
       "        [ 2.7879, -2.9002],\n",
       "        [ 2.2976, -2.3713]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fred0\\Anaconda3\\lib\\site-packages\\captum\\_utils\\gradient.py:59: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 1, 5, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = Gradient(ann)\n",
    "grad_exp = grad.get_explanation(inputs, labels).numpy()\n",
    "grad_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3200, 32, 1, 1])\n",
      "torch.Size([3200, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 1, 5, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Testing IG on Gaussian Data\n",
    "'''\n",
    "ig = IntegratedGradients(ann)\n",
    "exp_ig = ig.get_explanation(inputs, labels).detach().numpy()\n",
    "exp_ig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 32])\n",
      "tensor([[[[0.3777, 0.0086, 0.0095, 0.0292, 0.0061],\n",
      "          [0.0562, 0.0182, 0.0260, 0.0294, 0.0125],\n",
      "          [0.0363, 0.0459, 0.0242, 0.1213, 0.0041],\n",
      "          [0.0135, 0.0177, 0.0068, 0.0417, 0.0064],\n",
      "          [0.0345, 0.0158, 0.0241, 0.0058, 0.0282]]],\n",
      "\n",
      "\n",
      "        [[[0.0189, 0.0171, 0.0055, 0.0045, 0.0072],\n",
      "          [0.0402, 0.0361, 0.2884, 0.0048, 0.0095],\n",
      "          [0.0311, 0.0890, 0.0514, 0.0187, 0.1203],\n",
      "          [0.0184, 0.0290, 0.0531, 0.0537, 0.0100],\n",
      "          [0.0442, 0.0050, 0.0121, 0.0281, 0.0038]]],\n",
      "\n",
      "\n",
      "        [[[0.2701, 0.0149, 0.0106, 0.0386, 0.0081],\n",
      "          [0.0213, 0.0343, 0.0105, 0.0320, 0.0107],\n",
      "          [0.1231, 0.1186, 0.0251, 0.0820, 0.0177],\n",
      "          [0.0143, 0.0180, 0.0093, 0.0555, 0.0153],\n",
      "          [0.0013, 0.0131, 0.0182, 0.0085, 0.0288]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0039, 0.0098, 0.0273, 0.2761, 0.0021],\n",
      "          [0.0262, 0.0139, 0.0182, 0.0228, 0.0262],\n",
      "          [0.0061, 0.1300, 0.0778, 0.0516, 0.0246],\n",
      "          [0.0381, 0.0194, 0.0123, 0.0240, 0.0147],\n",
      "          [0.0181, 0.0353, 0.0839, 0.0298, 0.0078]]],\n",
      "\n",
      "\n",
      "        [[[0.0351, 0.0336, 0.1036, 0.0397, 0.0149],\n",
      "          [0.0214, 0.0155, 0.0135, 0.3736, 0.0147],\n",
      "          [0.0024, 0.0274, 0.1063, 0.0156, 0.0229],\n",
      "          [0.0044, 0.0379, 0.0343, 0.0089, 0.0096],\n",
      "          [0.0252, 0.0136, 0.0163, 0.0070, 0.0026]]],\n",
      "\n",
      "\n",
      "        [[[0.0037, 0.1857, 0.0396, 0.0453, 0.0485],\n",
      "          [0.0104, 0.0139, 0.0185, 0.0500, 0.0315],\n",
      "          [0.0228, 0.0422, 0.1257, 0.2471, 0.0103],\n",
      "          [0.0049, 0.0076, 0.0055, 0.0105, 0.0184],\n",
      "          [0.0147, 0.0102, 0.0221, 0.0063, 0.0042]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 1, 5, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Testing EBP on Gaussian Data\n",
    "'''\n",
    "ebp = EBP(ann)\n",
    "exp_ebp = ebp.get_explanation(inputs, labels).numpy()\n",
    "exp_ebp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 32])\n",
      "torch.Size([64, 32, 1, 1])\n",
      "torch.Size([64, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 5, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Testing LRP on Gaussian Data\n",
    "'''\n",
    "lrp = LRP(ann)\n",
    "exp_lrp = lrp.get_explanation(inputs, labels)\n",
    "exp_lrp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-01379be2fa41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m '''\n\u001b[0;32m      4\u001b[0m \u001b[0mggc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGuidedGradCAM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mann\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL_relu3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mggc_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mggc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_explanation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mggc_exp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\test2\\XAI-benchmark\\xai-benchmark\\explainers\\guided_gradcam.py\u001b[0m in \u001b[0;36mget_explanation\u001b[1;34m(self, x, label, interpolate_mode)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mggc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGGC_Captum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mattribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mggc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolate_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolate_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mattribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\captum\\log\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\captum\\attr\\_core\\guided_grad_cam.py\u001b[0m in \u001b[0;36mattribute\u001b[1;34m(self, inputs, target, additional_forward_args, interpolate_mode, attribute_to_layer_input)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0madditional_forward_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mrelu_attributions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         )\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_cam_attr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\captum\\attr\\_core\\layer\\grad_cam.py\u001b[0m in \u001b[0;36mattribute\u001b[1;34m(self, inputs, target, additional_forward_args, attribute_to_layer_input, relu_attributions)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[0madditional_forward_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0mdevice_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         )\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\captum\\_utils\\gradient.py\u001b[0m in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[1;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn)\u001b[0m\n\u001b[0;32m    598\u001b[0m             \u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mforward_hook_with_return\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m             \u001b[0mrequire_layer_grads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m         )\n\u001b[0;32m    602\u001b[0m         assert output[0].numel() == 1, (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\captum\\_utils\\gradient.py\u001b[0m in \u001b[0;36m_forward_layer_distributed_eval\u001b[1;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[0mall_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msingle_layer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mattribute_to_layer_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                 all_hooks.append(\n",
      "\u001b[1;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing GuidedGradCAM on Gaussian Data\n",
    "'''\n",
    "ggc = GuidedGradCAM(ann, ann.L_relu3)\n",
    "ggc_exp = ggc.get_explanation(inputs, labels)\n",
    "ggc_exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing GradCAM on Gaussian Data\n",
    "'''\n",
    "gradcam = GradCAM(ann, ann.conv)\n",
    "exp_gradcam = gradcam.get_explanation(inputs, labels)\n",
    "exp_gradcam.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
